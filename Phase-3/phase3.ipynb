{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data onto the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/deep_learning/Project\n"
     ]
    }
   ],
   "source": [
    "#%cd drive/My Drive/deep_learning/Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls drive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: 'train_ende.zip' and './train_ende.zip' are the same file\n",
      "cp: 'test.zip' and './test.zip' are the same file\n"
     ]
    }
   ],
   "source": [
    "# !cp train_ende.zip .\n",
    "# !cp test.zip ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MSCTD'...\n",
      "remote: Enumerating objects: 1217, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 1217 (delta 13), reused 7 (delta 3), pack-reused 1190\u001b[K\n",
      "Receiving objects: 100% (1217/1217), 102.24 MiB | 16.05 MiB/s, done.\n",
      "Resolving deltas: 100% (617/617), done.\n",
      "Updating files: 100% (934/934), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/XL2248/MSCTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp MSCTD/MSCTD_data/ende/english_*.txt .\n",
    "# !cp MSCTD/MSCTD_data/ende/image_index_*.txt .\n",
    "# !cp MSCTD/MSCTD_data/ende/sentiment_*.txt .\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "for file in os.listdir('MSCTD/MSCTD_data/ende'):\n",
    "    if file.startswith('english_'):\n",
    "        shutil.copy('MSCTD/MSCTD_data/ende/' + file, file)\n",
    "    if file.startswith('image_index_'):\n",
    "        shutil.copy('MSCTD/MSCTD_data/ende/' + file, file)\n",
    "    if file.startswith('sentiment_'):\n",
    "        shutil.copy('MSCTD/MSCTD_data/ende/' + file, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown\n",
    "!gdown --id 1GAZgPpTUBSfhne-Tp0GDkvSHuq6EMMbj\n",
    "!gdown --id 1B9ZFmSTqfTMaqJ15nQDrRNLqBvo-B39W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for x in *.zip\n",
    "do\n",
    "  unzip -qq $x\n",
    "done;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir dataset\n",
    "# !cd dataset; mkdir train test dev\n",
    "\n",
    "os.makedirs('dataset', exist_ok=True)\n",
    "os.makedirs('dataset/train', exist_ok=True)\n",
    "os.makedirs('dataset/test', exist_ok=True)\n",
    "os.makedirs('dataset/dev', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mv *train* dataset/train\n",
    "# !mv *test* dataset/test\n",
    "# !mv *dev* dataset/dev\n",
    "\n",
    "for file in os.listdir():\n",
    "    if 'train' in file:\n",
    "        shutil.move(file, 'dataset/train')\n",
    "    if 'test' in file:\n",
    "        shutil.move(file, 'dataset/test')\n",
    "    if 'dev' in file:\n",
    "        shutil.move(file, 'dataset/dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import linecache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSCTD_Dataset (Dataset):\n",
    "  def __init__(self, dataset_dir, images_dir, conversation_dir, texts, sentiments,\n",
    "                transform=None, preprocess_func=None, pad_idx=None, max_len=None):\n",
    "    self.dataset_path = Path(dataset_dir)\n",
    "    self.images_path = self.dataset_path / images_dir\n",
    "    self.sentiment_path = self.dataset_path / sentiments\n",
    "    self.text_path = self.dataset_path / texts\n",
    "    self.conversations_path = self.dataset_path / conversation_dir\n",
    "\n",
    "    self.transform = transform\n",
    "\n",
    "    self.preprocess_func = preprocess_func\n",
    "    self.pad_idx = pad_idx\n",
    "    self.max_len = max_len\n",
    "\n",
    "    with open(self.text_path, 'r') as f:\n",
    "        self.texts = f.read().splitlines()\n",
    "\n",
    "    with open(self.sentiment_path, 'r') as f:\n",
    "        self.sentiments = np.array(f.read().splitlines()).astype(\"int32\")\n",
    "\n",
    "    with open(self.conversations_path, 'r') as f:\n",
    "        self.conversations = np.array(f.read().splitlines())\n",
    "    \n",
    "  def __len__(self):\n",
    "        return len(self.sentiments)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "        img_path = self.images_path / f'{idx}.jpg'\n",
    "        image = Image.open(img_path)\n",
    "        # image = read_image(str(img_path))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "       \n",
    "        text = self.texts[idx].strip()\n",
    "\n",
    "        if self.preprocess_func is not None:\n",
    "            text = self.preprocess_func(text)\n",
    "            if self.max_len is not None:\n",
    "                text = text[:self.max_len]\n",
    "            if self.pad_idx is not None:\n",
    "                text = F.pad(torch.tensor(text), (0, self.max_len - len(text)), 'constant', self.pad_idx)\n",
    "        \n",
    "        sentiment = self.sentiments[idx]\n",
    "\n",
    "        data_dict = {\"text\":text,\n",
    "                     \"image\":image,\n",
    "                     \"sentiment\":sentiment}\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = MSCTD_Dataset('dataset/train', 'train_ende', 'image_index_train.txt', 'english_train.txt', 'sentiment_train.txt')\n",
    "# do we have all parts of devset?\n",
    "devset = MSCTD_Dataset('dataset/dev', 'dev_ende', 'image_index_dev.txt', 'english_dev.txt', 'sentiment_dev.txt')\n",
    "testset = MSCTD_Dataset('dataset/test', 'test_ende', 'image_index_test.txt', 'english_test.txt', 'sentiment_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(devset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Congfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer\n",
    "from transformers import BertModel, AutoModel, BertForSequenceClassification\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general config\n",
    "MAX_LEN = 25\n",
    "\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VALID_BATCH_SIZE = 256\n",
    "TEST_BATCH_SIZE = 256\n",
    "\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "MODEL_NAME = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e51c8f30df4377b0d95e1ace0b6104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab52e31723d4c6c9272bab654f5c5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0760e42fdd405f8ad93e429b33ec8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "config = BertConfig.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "\n",
    "    def __init__(self, tokenizer=tokenizer, num_classes=3, max_len=MAX_LEN) -> None:\n",
    "        super().__init__()\n",
    "        self.cnn = None  # dfine the CNN model\n",
    "        self.cnn.requires_grad_(False)\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            num_labels = 3,\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False,\n",
    "        )\n",
    "        self.bert.requires_grad_(False)\n",
    "        # size of concatenated vector (this may raise error)\n",
    "        self.embedding_size = self.cnn(torch.rand(1, 3, 224, 224)).shape[1] + self.bert(torch.rand(1, 25))['pooler_output'].shape[1]\n",
    "        self.fc = nn.Linear(self.embedding_size, num_classes)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image = self.cnn(image)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt', \n",
    "        )\n",
    "        # if they are of shape (N, 1, D), then they should be squeezed to (N, D)\n",
    "        input_ids = encoding['input_ids'].flatten().to(device)\n",
    "        attention_mask = encoding['attention_mask'].flatten().to(device)\n",
    "        token_type_ids = encoding['token_type_ids'].flatten().to(device)\n",
    "        x = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        pooler_output = x['pooler_output']\n",
    "        x = torch.cat((image, pooler_output), dim=1)\n",
    "        x = self.fc(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def one_epoch(model, loader, criterion, optimizer=None, epoch='', train=True, set_name='Train', metrics=None):\n",
    "    total_loss = 0\n",
    "    N = len(loader.dataset)\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(train), tqdm.tqdm(enumerate(loader), total=len(loader)) as pbar:\n",
    "        for i, data_i in pbar:\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            image, text, y = data_i['image'], data_i['text'], data_i['sentiment']\n",
    "            image = image.to(device)\n",
    "            text = text.to(device)\n",
    "            y = y.long().to(device)\n",
    "\n",
    "            p = model(image, text)\n",
    "\n",
    "            loss = criterion(p, y.long())\n",
    "\n",
    "            total_loss += loss.item() * len(y)\n",
    "            pbar.set_description(f'{epoch}: {set_name} Loss: {total_loss / N:.3e}')\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            y_pred = p.argmax(dim=-1)\n",
    "            Y.append(y.cpu().numpy())\n",
    "            Y_pred.append(y_pred.cpu().numpy())\n",
    "\n",
    "    total_loss /= N\n",
    "\n",
    "    Y = np.concatenate(Y)\n",
    "    Y_pred = np.concatenate(Y_pred)\n",
    "    acc = accuracy_score(Y_pred, Y)\n",
    "    print(f'Accuracy of {set_name} set: {acc}')\n",
    "\n",
    "    result = {'loss': total_loss, 'accuracy': acc}\n",
    "    if metrics is not None:\n",
    "        result.update({metric: metric_func(Y, Y_pred) for metric, metric_func in metrics.items()})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, num_epochs, criterion, optimizer, model_name='pytroch-model', scheduler=None):\n",
    "    train_loader, val_loader = dataloaders\n",
    "    min_val_loss = np.inf\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        result = one_epoch(model, train_loader, criterion, optimizer, epoch, train=True, set_name='Train')\n",
    "        train_loss = result['loss']\n",
    "        train_acc = result['accuracy']\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_result = one_epoch(model, val_loader, criterion, epoch=epoch, train=False, set_name='Validation')\n",
    "        val_loss = val_result['loss']\n",
    "        val_acc = val_result['accuracy']\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print('\\n', '-' * 60)\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'{model_name}.pt')\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(val_losses, label='val')\n",
    "    plt.title('loss history of training and val sets')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_accuracies, label='train')\n",
    "    plt.plot(val_accuracies, label='val')\n",
    "    plt.title('Accuracy history of training and val sets')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model.load_state_dict(torch.load(f'{model_name}.pt'))\n",
    "    return model, min_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModalModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCH = 20\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, verbose=True, factor=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model, min_val_loss = train_model(model, (train_loader, dev_loader), EPOCH, criterion, optimizer, model_name='bert_cnn', scheduler=scheduler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_policy = 'macro'\n",
    "metrics = {'accuracy': accuracy_score, 'precision': lambda y1, y2: precision_score(y1, y2, average=average_policy),\n",
    "           'recall': lambda y1, y2: recall_score(y1, y2, average=average_policy),\n",
    "           'f1': lambda y1, y2: f1_score(y1, y2, average=average_policy),\n",
    "           'confusion_matrix': confusion_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, loader, metrics=metrics, set_name='Test', plot_confusion_matrix=True):\n",
    "    results = one_epoch(model, loader, criterion, train=False, set_name=set_name, metrics=metrics)\n",
    "    disp = ConfusionMatrixDisplay(results.pop('confusion_matrix'))\n",
    "    if plot_confusion_matrix:\n",
    "        disp.plot()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67bfac4f4aefe1c16f1836a62d55b6e6baa7aba1ac5ce70e93ee8e90eb4f073a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
